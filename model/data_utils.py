# æ­¤å¤„éƒ¨åˆ†ä»£ç éƒ¨åˆ†æ¥æºäº
# https://github.com/tensorflow/models/tree/master/tutorials/rnn/translate
# æœ‰æ”¹åŠ¨å’Œæ·»åŠ  å› ä¸ºæ˜¯æºç æ”¹åŠ¨ æ‰€ä»¥æ²¡æœ‰import å€Ÿé‰´çš„éƒ¨åˆ†çš„é“¾æ¥å¦‚ä¸Š
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import os
import re

from tensorflow.python.platform import gfile
import tensorflow as tf
import jieba

_PAD = "_PAD"
_GO = "_GO"
_EOS = "_EOS"
_UNK = "_UNK"
_DIG = "_DIG"
_START_VOCAB = [_PAD, _GO, _EOS, _UNK]

PAD_ID = 0
GO_ID = 1
EOS_ID = 2
UNK_ID = 3

# ï¼Ÿï¼ã€Šã€‹{}-â€”â€”+=ã€ å­—ç¬¦æ•°å­—çš„å»é™¤
_WORD_SPLIT = re.compile("([.,!?\"':;ï¼Œã€‚ï¼›â€œâ€˜â€™â€ï¼š)(])")
_DIGIT_RE = re.compile(r"\d")
# æˆ‘å»æ‰äº†ç¬¦å·?!ï¼Ÿï¼ è¿™ä¸¤ä¸ªå¾ˆå¸¸ç”¨ä¸”èƒ½è¡¨æ„
_SPECIAL_CHAR = " \t\\,./;'\"" \
                "[]`~@#$%^&*()_+-={}:<>" \
                "ï½~Â·@#ï¿¥%â€¦â€¦&*ï¼ˆï¼‰â€”â€”ã€ã€‘ï¼šâ€œâ€ï¼›â€™â€˜ã€Šã€‹ï¼Œã€‚ã€" \
                "abcdefghijklmnopqrstuvwxyz" \
                "ABCDEFGHIJKLMNOPQRSTUVWXYZ" \
                "ã°ã‚Œã‚„ã†ã—ãƒ¾ãƒ˜ã¦ã¾ã«ã¥ã‚‰ã§ãƒã¤ã‚¥ãƒ½ã™ã‚Šã£ã„ï½€ãŒã®ã‹ãˆã‚ˆã¿ã‚“ãŠã­ï½¯ï½¸ï¾" \
                "â‘ â‘¡â‘¢â‘£ï½ï½„ï½…Ğµï½‡ï½ˆĞ½ï½‹ï½ï½Î¿ï¼±ï½”ï¼µğŸ‡®ÑÃ—Ğ°" \
                "Î±ï½Î˜" \
                "ï¸ºï¸¹ã€â””ã€Œâ”â”‘â•¯â•°â•­â•®ï¸¶â€¿ï¼ï¼¼ã€‰|âˆ ï¼ã€‰ï¸¿â”€ï¼¿ï¿£ï¼â—¡âŒ’" \
                "Î£â‰¤â‰¥â‰§â‰¦Ñ”ÎµĞ·â†â†’â†“â‰¡âŠ‚âŠƒâˆ©âˆ€âŠ™â–½âˆ‡â–³âˆˆâ‰ â€º" \
                "ï¼‚îˆã€€â—‰â—•ï½¡à²¥âÏ‰â™¡ã…‚ï½Œï¼ï¹Â´" \
                "Ô…Ñ‹â€¢â…±Ğ´Ğ»ÑˆÙˆÎ ï¿½ï¼â—ï½¥à¸…Â°âœ§ãƒ»â–¡ã€ƒï¼…â„ƒï¼ â˜â˜œâœ²ï¾Ÿã€œË˜Ë‡Ã©ï¸´ã‚œï¼ƒ"
_MYDICT = r"../resource/mydict.txt"


def basic_tokenizer(sentence, chs=True, char_level=False):
    """
    åŸºæœ¬åˆ†è¯å·¥å…·
    :param sentence: æºè¯­å¥
    :param chs: TRUE æ±‰è¯­ä¸”æœªåˆ†è¯ FALSE åŸºäºç©ºæ ¼åˆ†è¯
    :param char_level: æ˜¯å¦åŸºäºå­—ç¬¦
    :return: è¯åˆ—è¡¨
    """
    if char_level:
        return [x for x in sentence.strip() if x not in _SPECIAL_CHAR]
    else:
        # è°ƒç”¨ç»“å·´çš„ä¸­æ–‡åˆ†è¯
        if chs:
            sentence = sentence.strip()
            words_list = jieba.cut(sentence, cut_all=False)
            return [x.lower() for x in words_list if x not in _SPECIAL_CHAR]
        # éä¸­æ–‡ åŸºäºç©ºæ ¼åˆ†è¯
        else:
            words = []
            for space_separated_fragment in sentence.strip().split():
                words.extend(_WORD_SPLIT.split(space_separated_fragment))
            return [w.lower() for w in words if w]


def create_vocabulary(vocabulary_path, data_path_list,
                      max_character_size, tokenizer=None,
                      chs=True, char_level=False,
                      normalize_digits=True):
    """
    åˆ›å»ºè¯æ±‡è¡¨æ–‡ä»¶ï¼ˆè‹¥ä¸å­˜åœ¨ï¼‰
    :param vocabulary_path: å­˜å‚¨ä½ç½®
    :param data_path_list: ç”¨äºåˆ›å»ºè¯æ±‡è¡¨çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    :param max_character_size: æœ‰æ•ˆè¯æ±‡æ•° ä¿ç•™è¯æ±‡æœ€å¤§å€¼
    :param tokenizer: åˆ†è¯å™¨
    :param chs: ä¸­æ–‡æ¨¡å¼ï¼ˆæœªåˆ†è¯ä¸­æ–‡ï¼‰
    :param char_level: åŸºäºå­—ç¬¦
    :param normalize_digits: æŠŠæ•°å­—å‡æ›¿æ¢ä¸º_DIG
    :return:
    """
    # data_path æ˜¯ä¸ªè·¯å¾„çš„æ•°ç»„
    if not gfile.Exists(vocabulary_path):
        print("åˆ›å»ºè¯æ±‡è¡¨")
        vcb = {}
        for path in data_path_list:
            with gfile.GFile(path, mode="r") as f:
                print("å¤„ç†%sæ–‡ä»¶" % path)
                counter = 0
                for line in f:
                    counter += 1
                    if counter % 10000 == 0:
                        print("å·²å¤„ç†%sè¡Œ" % counter)
                    line = tf.compat.as_text(line)
                    tokens = tokenizer(line) if tokenizer else basic_tokenizer(line, chs, char_level)
                    for token in tokens:
                        word = _DIGIT_RE.sub(_DIG, token) if normalize_digits else token
                        if word in vcb:
                            vcb[word] += 1
                        else:
                            vcb[word] = 1
        vocab_list = _START_VOCAB + sorted(vcb, key=vcb.get, reverse=True)
        if len(vocab_list) > max_character_size:
            vocab_list = vocab_list[:max_character_size]
        with gfile.GFile(vocabulary_path, mode="w") as vocabulary_file:
            for w in vocab_list:
                vocabulary_file.write(w + "\n")
    else:
        print("è¯æ±‡è¡¨å·²å­˜åœ¨ï¼Œå¦‚æœä¸å¯¹åº”è¯·åˆ é™¤åå†æ¬¡è¿è¡Œã€‚")


def initialize_vocabulary(vocabulary_path):
    """
    ä»è¯æ±‡è¡¨æ–‡ä»¶è¯»å–è¯æ±‡è¡¨
    :param vocabulary_path:
    :return: dict[è¯ï¼Œæ•°å­—] and list[è¯]
    """
    if gfile.Exists(vocabulary_path):
        rev_vocab = []
        with gfile.GFile(vocabulary_path, mode="r") as f:
            rev_vocab.extend(f.readlines())
        rev_vocab = [tf.compat.as_text(line.strip()) for line in rev_vocab]
        vocab = dict([(x, y) for (y, x) in enumerate(rev_vocab)])
        return vocab, rev_vocab
    else:
        raise ValueError("Vocabulary file %s not found.", vocabulary_path)


def sentence_to_token_ids(sentence, vocabulary,
                          tokenizer=None, chs=True, char_level=False,
                          normalize_digits=True):
    """
    è¿”å›ç”±IDç»„æˆçš„å¥å­
    :param sentence:
    :param vocabulary: è¯æ±‡è¡¨ [è¯ï¼Œ æ•°å­—]
    :param tokenizer:
    :param normalize_digits:
    :param chs:
    :param char_level:
    :return: è¯æ±‡IDçš„åˆ—è¡¨
    """
    if tokenizer:
        words = tokenizer(sentence)
    else:
        words = basic_tokenizer(sentence, chs, char_level)
    if not normalize_digits:
        return [vocabulary.get(w, UNK_ID) for w in words]
    return [vocabulary.get(_DIGIT_RE.sub(_DIG, w), UNK_ID) for w in words]


def data_to_token_ids(data_path, target_path, vocabulary_path,
                      tokenizer=None, chs=True, char_level=False,
                      normalize_digits=True):
    if not gfile.Exists(target_path):
        print("Tokenizing data in %s" % data_path)
        vocab, _ = initialize_vocabulary(vocabulary_path)
        with gfile.GFile(data_path, mode="r") as data_file:
            with gfile.GFile(target_path, mode="w") as tokens_file:
                counter = 0
                for line in data_file:
                    counter += 1
                    if counter % 10000 == 0:
                        print("  tokenizing line %d" % counter)
                    token_ids = sentence_to_token_ids(tf.compat.as_text(line), vocab,
                                                      tokenizer,
                                                      chs=chs, char_level=char_level,
                                                      normalize_digits=normalize_digits)
                    tokens_file.write(" ".join([str(tok) for tok in token_ids]) + "\n")
    else:
        print("Tokenizing data is existed.(%s)" % target_path)


def prepare_data(data_dir, from_train_path, to_train_path, from_dev_path, to_dev_path,
                 vocabulary_size, tokenizer=None, chs=True, char_level=False):
    """
    æŠŠè®­ç»ƒæ–‡ä»¶å¤¹ä¸‹çš„æ–‡ä»¶è½¬åŒ–æˆtoken idæ–‡ä»¶
    :param data_dir:
    :param from_train_path:
    :param to_train_path:
    :param from_dev_path:
    :param to_dev_path:
    :param vocabulary_size:
    :param tokenizer:
    :param chs:
    :param char_level:
    :return:    (å¯¹è¯å…ˆè¯´è®­ç»ƒidè¡¨ç¤ºï¼Œå¯¹è¯åº”ç­”è®­ç»ƒidè¡¨ç¤ºï¼Œ
                å¯¹è¯å…ˆè¯´æµ‹è¯•idè¡¨ç¤ºï¼Œ å¯¹è¯åº”ç­”æµ‹è¯•idè¡¨ç¤ºï¼Œ
                è¯æ±‡è¡¨è·¯å¾„)
    """
    postfix = ".%s.%s.ids%d" % ("zh" if chs else "en", "ch" if char_level else "vcb", vocabulary_size)
    vocab_path = os.path.join(data_dir, "vocabulary%d.txt" % vocabulary_size)
    create_vocabulary(vocab_path, [from_train_path, to_train_path], vocabulary_size, tokenizer,
                      chs=chs, char_level=char_level)
    
    to_train_ids_path = to_train_path + postfix
    from_train_ids_path = from_train_path + postfix
    data_to_token_ids(to_train_path, to_train_ids_path, vocab_path, tokenizer, chs=chs, char_level=char_level)
    data_to_token_ids(from_train_path, from_train_ids_path, vocab_path, tokenizer, chs=chs, char_level=char_level)
    
    to_dev_ids_path = to_dev_path + postfix
    from_dev_ids_path = from_dev_path + postfix
    data_to_token_ids(to_dev_path, to_dev_ids_path, vocab_path, tokenizer, chs=chs, char_level=char_level)
    data_to_token_ids(from_dev_path, from_dev_ids_path, vocab_path, tokenizer, chs=chs, char_level=char_level)
    return (from_train_ids_path, to_train_ids_path,
            from_dev_ids_path, to_dev_ids_path,
            vocab_path)


def train_text_to_ids(para, vcb_sz=None, ch_lvl=None, chs_md=None):
    """
    å°†å‚æ•°ä¸­çš„åœ°å€æŒ‡å®šçš„æ–‡ä»¶è½¬æ¢æˆç”±idè¡¨ç¤ºã€‚
    :param vcb_sz:
    :param ch_lvl:
    :param chs_md:
    :return:
    """
    from_train = None
    to_train = None
    from_dev = None
    to_dev = None
    vocab_path = None
    vocabulary_size = para.vocabulary_size if vcb_sz is None else vcb_sz
    ch_level = para.ch_level if ch_lvl is None else ch_lvl
    chs_mode = para.chs_mode if chs_md is None else chs_md
    if para.from_train_data and para.to_train_data:
        from_train_data = para.from_train_data
        to_train_data = para.to_train_data
        from_dev_data = from_train_data
        to_dev_data = to_train_data
        if para.from_dev_data and para.to_dev_data:
            from_dev_data = para.from_dev_data
            to_dev_data = para.to_dev_data
        from_train, to_train, from_dev, to_dev, vocab_path = prepare_data(
            para.data_dir,
            from_train_data, to_train_data,
            from_dev_data, to_dev_data,
            vocabulary_size, chs=chs_mode, char_level=ch_level)
        return from_train, to_train, from_dev, to_dev, vocab_path
    else:
        print("Please specify the file path in %s" % para.data_dir)
        return from_train, to_train, from_dev, to_dev, vocab_path


def read_data_from_many_id_files(paths):
    data = []
    for filepath in paths:
        if gfile.Exists(filepath):
            with gfile.GFile(filepath, mode="r") as f:
                line = f.readline()
                while line:
                    data.extend([int(x) for x in line.split()])
                    data.append(EOS_ID)
                    line = f.readline()
        else:
            raise ValueError("file %s not found.", filepath)
    return data


def read_sentences_from_many_id_files(paths):
    sentence_list = []
    for filepath in paths:
        if gfile.Exists(filepath):
            with gfile.GFile(filepath, mode='r') as f:
                line = f.readline()
                # è‹¥è¯»å‡ºäº†ä¸€è¡Œ
                while line:
                    # è¿™è¡Œæ•°æ®è½¬æ¢æˆidçš„list
                    sentence = [int(x) for x in line.split()]
                    # ä¸ä¸ºç©ºåˆ™æ·»åŠ è¿™è¡Œæ•°æ® ä¿è¯æ²¡æœ‰ç©ºçš„æ•°æ®
                    if len(sentence) > 0:
                        sentence_list.append(sentence)
                    # ä¸‹ä¸€è¡Œ
                    line = f.readline()
        else:
            raise ValueError("file %s not found.", filepath)
    return sentence_list
